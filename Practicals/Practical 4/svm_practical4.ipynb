{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Student:        Byron Dowling\n",
    "    Class:          Computer Vision (CSE 60535)\n",
    "    Term:           University of Notre Dame, Fall 2023\n",
    "    Assignment:     Practical #4 : Deep Learning-based Object Detection\n",
    "\"\"\"\n",
    "\n",
    "# Two lines below (when uncommented) allow you to track the time spent on each cell, if you wanted\n",
    "!pip install ipython-autotime\n",
    "%load_ext autotime\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import Compose, Resize\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to tasks 1-4:\n",
    "\n",
    "### Task 1\n",
    "- The dollar bill was recognized fairly quickly at different angle\n",
    "- My cellphone was recognized fairly well, but this was a bit ambiguous because the camera also thought that I was a cell phone\n",
    "- My watch took the longest to identify and it only occured at close range, and this could be because it is a smart watch so it shares some characteristics with a smartphone\n",
    "\n",
    "### Task 2\n",
    "- The linear SVM when changed to fc1 performed considerably worse than when on fc2\n",
    "- My cellphone was recognized fine, my watch was not recognized at all, and the dollar bill was only recognized some of the time, often it was misclassified as a cellphone\n",
    "- The combination of fc1 and a polynomial kernel is the worst performer so far. It was only able to correctly identify a watch, and that in of itself could be flawed as it thought I was a watch and nothing presented to the camera made it change its mind including my phone or a dollar bill.\n",
    "- I experienced the same poor results above when I tried the combination of fc2 and a polynomial kernel, only a watch was detected, but this was flawed because the background/me was identified as a watch and it never changed off of this\n",
    "- When testing fc1 and fc2 on a rbf, I did not see signifcant performance increase. They did recognize my cellphone if I presented it with the back camera facing the webcam\n",
    "\n",
    "### Task 3\n",
    "- When adding in extra objects on the configuration from task 2, the addition of more objects decreased performance.\n",
    "- I suspect this is from having more options to choose from and the ability for more items to share similar features and characteristics\n",
    "\n",
    "### Task 4 (for 60000-level section)\n",
    "- Ran this in a different Colab: https://colab.research.google.com/drive/14JskQTdA-TMOirTmLXFH45-asuKXP9LH?usp=sharing \n",
    "- So far my best performing set up was:\n",
    "    - Two Dense Layers with 128 nodes and relu activation\n",
    "    - Two drop out layers at 0.25%\n",
    "    - A final Dense layer of 101 neurons and softmax\n",
    "    - 10 Epochs\n",
    "    - Accuracy was around 92%\n",
    "\n",
    "- I experimented with a few more setups but performance was slightly worse and stagnated\n",
    "    - Particularly when I used my current best setup and extended the Epochs from 10 to 15, minimal increase was observed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are utilizing Keras API to load the pretrained VGG16 model as our feature extractor for training SVM\n",
    "model = VGG16(weights='imagenet')\n",
    "\n",
    "# *** TASK 2 ***\n",
    "# Choose the layer of the VGG model used to get your features (= \"network embeddings\")\n",
    "cnn_codes = 'fc2'\n",
    "\n",
    "# Loading our model that will output the network enbeddings specified by us above (instead of a classification decision)\n",
    "features_model = Model(inputs=model.input, outputs=model.get_layer(cnn_codes).output)\n",
    "\n",
    "# And here is the function that will give us VGG-based features for an image \"img\"\n",
    "def extract_vgg_features(img):\n",
    "    # prepare the image for VGG\n",
    "    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "    img = img[np.newaxis, :, :, :]\n",
    "    # call feature extraction\n",
    "    return features_model.predict(img,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's download and extract the Caltech101 dataset.\n",
    "!wget -c https://notredame.box.com/shared/static/o5hw6ljq7x00smui4ixo9akxwlq2dkib.gz -O caltech101.tar.gz\n",
    "%mkdir ./caltech101/\n",
    "!tar -zxf caltech101.tar.gz -C ./caltech101/\n",
    "\n",
    "# We need to convert PIL images in to OpenCV images\n",
    "class ToCV2(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        #image, label = sample['image'], sample['label']\n",
    "\n",
    "        image = np.array(sample.convert('RGB'))\n",
    "        image = image[:, :, ::-1].copy()\n",
    "\n",
    "        return image\n",
    "\n",
    "caltech101_dataset = datasets.Caltech101(root='./', transform=Compose([Resize((224, 224)), ToCV2()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let us find the label numbers for our objects\n",
    "\n",
    "# *** TASK 1 *** and *** TASK 3 ***\n",
    "#  We are going to use this list to restrict the objets our classifier will recognize\n",
    "# my_object_list = ['watch','cellphone','dollar_bill']\n",
    "my_object_list = ['watch','cellphone','dollar_bill','lotus','camera','stapler','celing_fan','headphone','scissors','lobster']\n",
    "\n",
    "my_object_label_list = []\n",
    "for my_object in my_object_list:\n",
    "    for i, category in enumerate(caltech101_dataset.categories):\n",
    "        if category == my_object:\n",
    "            my_object_label_list.append(i)\n",
    "print(\"{} -> {} (class indicies in CALTECH101)\".format(my_object_list,my_object_label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let's extract the VGG features required for our selected object categories.\n",
    "# It should take no longer than 5 minutes.\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "c101_images = []\n",
    "c101_labels = []\n",
    "c101_label_to_i = {}\n",
    "\n",
    "# We can load the entire dataset on RAM as the dataset is pretty small\n",
    "print(\"Loading dataset to RAM....\")\n",
    "for image, label in tqdm(caltech101_dataset):\n",
    "    if int(label) in my_object_label_list:\n",
    "        c101_images.append(image)\n",
    "        c101_labels.append(int(label))\n",
    "\n",
    "print('Extracting features.....')\n",
    "# Now we convert the images into vgg features\n",
    "c101_vgg_features = features_model.predict(np.array(c101_images), batch_size=batch_size, workers=2, use_multiprocessing=True, verbose=1)\n",
    "\n",
    "assert len(c101_vgg_features) == len(c101_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** TASK 2 ***\n",
    "clf = svm.SVC(kernel='linear').fit(c101_vgg_features, c101_labels)\n",
    "# clf = svm.SVC(kernel='poly',degree=3).fit(c101_vgg_features, c101_labels)\n",
    "# clf = svm.SVC(kernel='rbf',gamma='auto').fit(c101_vgg_features, c101_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Javascript, Image\n",
    "from google.colab.output import eval_js\n",
    "from google.colab.patches import cv2_imshow\n",
    "from base64 import b64decode, b64encode\n",
    "import PIL\n",
    "import io\n",
    "\n",
    "# function to convert the JavaScript object into an OpenCV image\n",
    "def js_to_image(js_reply):\n",
    "  \"\"\"\n",
    "  Params:\n",
    "          js_reply: JavaScript object containing image from webcam\n",
    "  Returns:\n",
    "          img: OpenCV BGR image\n",
    "  \"\"\"\n",
    "  # decode base64 image\n",
    "  image_bytes = b64decode(js_reply.split(',')[1])\n",
    "  # convert bytes to numpy array\n",
    "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
    "  # decode numpy array into OpenCV BGR image\n",
    "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
    "\n",
    "  return img\n",
    "\n",
    "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
    "def to_bytes(bbox_array):\n",
    "  \"\"\"\n",
    "  Params:\n",
    "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
    "  Returns:\n",
    "        bytes: Base64 image byte string\n",
    "  \"\"\"\n",
    "  # convert array into PIL image\n",
    "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
    "  iobuf = io.BytesIO()\n",
    "  # format bbox into png for return\n",
    "  bbox_PIL.save(iobuf, format='png')\n",
    "  # format return string\n",
    "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
    "\n",
    "  return bbox_bytes\n",
    "\n",
    "#@title Webcam live streaming code (just run it)\n",
    "# JavaScript to properly create our live video stream using our webcam as input\n",
    "def video_stream():\n",
    "  js = Javascript('''\n",
    "    var video;\n",
    "    var div = null;\n",
    "    var stream;\n",
    "    var captureCanvas;\n",
    "    var imgElement;\n",
    "    var labelElement;\n",
    "\n",
    "    var pendingResolve = null;\n",
    "    var shutdown = false;\n",
    "\n",
    "    function removeDom() {\n",
    "       stream.getVideoTracks()[0].stop();\n",
    "       video.remove();\n",
    "       div.remove();\n",
    "       video = null;\n",
    "       div = null;\n",
    "       stream = null;\n",
    "       imgElement = null;\n",
    "       captureCanvas = null;\n",
    "       labelElement = null;\n",
    "    }\n",
    "\n",
    "    function onAnimationFrame() {\n",
    "      if (!shutdown) {\n",
    "        window.requestAnimationFrame(onAnimationFrame);\n",
    "      }\n",
    "      if (pendingResolve) {\n",
    "        var result = \"\";\n",
    "        if (!shutdown) {\n",
    "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
    "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
    "        }\n",
    "        var lp = pendingResolve;\n",
    "        pendingResolve = null;\n",
    "        lp(result);\n",
    "      }\n",
    "    }\n",
    "\n",
    "    async function createDom() {\n",
    "      if (div !== null) {\n",
    "        return stream;\n",
    "      }\n",
    "\n",
    "      div = document.createElement('div');\n",
    "      div.style.border = '2px solid black';\n",
    "      div.style.padding = '3px';\n",
    "      div.style.width = '100%';\n",
    "      div.style.maxWidth = '600px';\n",
    "      document.body.appendChild(div);\n",
    "\n",
    "      const modelOut = document.createElement('div');\n",
    "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
    "      labelElement = document.createElement('span');\n",
    "      labelElement.innerText = 'No data';\n",
    "      labelElement.style.fontWeight = 'bold';\n",
    "      modelOut.appendChild(labelElement);\n",
    "      div.appendChild(modelOut);\n",
    "\n",
    "      video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      video.width = div.clientWidth - 6;\n",
    "      video.setAttribute('playsinline', '');\n",
    "      video.onclick = () => { shutdown = true; };\n",
    "      stream = await navigator.mediaDevices.getUserMedia(\n",
    "          {video: { facingMode: \"environment\"}});\n",
    "      div.appendChild(video);\n",
    "\n",
    "      imgElement = document.createElement('img');\n",
    "      imgElement.style.position = 'absolute';\n",
    "      imgElement.style.zIndex = 1;\n",
    "      imgElement.onclick = () => { shutdown = true; };\n",
    "      div.appendChild(imgElement);\n",
    "\n",
    "      const instruction = document.createElement('div');\n",
    "      instruction.innerHTML =\n",
    "          '<span style=\"color: red; font-weight: bold;\">' +\n",
    "          'When finished, click here or on the video to stop this demo</span>';\n",
    "      div.appendChild(instruction);\n",
    "      instruction.onclick = () => { shutdown = true; };\n",
    "\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      captureCanvas = document.createElement('canvas');\n",
    "      captureCanvas.width = 640; //video.videoWidth;\n",
    "      captureCanvas.height = 480; //video.videoHeight;\n",
    "      window.requestAnimationFrame(onAnimationFrame);\n",
    "\n",
    "      return stream;\n",
    "    }\n",
    "    async function stream_frame(label, imgData) {\n",
    "      if (shutdown) {\n",
    "        removeDom();\n",
    "        shutdown = false;\n",
    "        return '';\n",
    "      }\n",
    "\n",
    "      var preCreate = Date.now();\n",
    "      stream = await createDom();\n",
    "\n",
    "      var preShow = Date.now();\n",
    "      if (label != \"\") {\n",
    "        labelElement.innerHTML = label;\n",
    "      }\n",
    "\n",
    "      if (imgData != \"\") {\n",
    "        var videoRect = video.getClientRects()[0];\n",
    "        imgElement.style.top = videoRect.top + \"px\";\n",
    "        imgElement.style.left = videoRect.left + \"px\";\n",
    "        imgElement.style.width = videoRect.width + \"px\";\n",
    "        imgElement.style.height = videoRect.height + \"px\";\n",
    "        imgElement.src = imgData;\n",
    "      }\n",
    "\n",
    "      var preCapture = Date.now();\n",
    "      var result = await new Promise(function(resolve, reject) {\n",
    "        pendingResolve = resolve;\n",
    "      });\n",
    "      shutdown = false;\n",
    "\n",
    "      return {'create': preShow - preCreate,\n",
    "              'show': preCapture - preShow,\n",
    "              'capture': Date.now() - preCapture,\n",
    "              'img': result};\n",
    "    }\n",
    "    ''')\n",
    "\n",
    "  display(js)\n",
    "\n",
    "\n",
    "def video_frame(label, bbox):\n",
    "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's see how our trained SVM processes the webcam stream\n",
    "\n",
    "video_stream()\n",
    "\n",
    "# label for video\n",
    "label_html = 'Capturing...'\n",
    "\n",
    "# initialze bounding box to empty string\n",
    "svm_overlay = ''\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    js_reply = video_frame(label_html, svm_overlay)\n",
    "    if not js_reply:\n",
    "        break\n",
    "\n",
    "    # convert JS response to OpenCV image\n",
    "    frame = js_to_image(js_reply[\"img\"])\n",
    "\n",
    "    # create transparent overlay for svm classification info\n",
    "    svm_overlay = np.zeros([480,640,4], dtype=np.uint8)\n",
    "\n",
    "    features = extract_vgg_features(frame)\n",
    "    pred = clf.predict(features)\n",
    "\n",
    "    # show the classification result\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(svm_overlay, '{}'.format(caltech101_dataset.categories[pred[0]]), (15, 25), font, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    svm_overlay[:,:,3] = (svm_overlay.max(axis = 2) > 0 ).astype(int) * 255\n",
    "    # convert overlay of bbox into bytes\n",
    "    svm_overlay = to_bytes(svm_overlay)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
